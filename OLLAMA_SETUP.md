# ğŸ¤– Configuration Ollama pour l'Assistant IA

## ğŸ“‹ PrÃ©requis

Pour que l'assistant IA fonctionne avec le LLM (Large Language Model), vous devez installer et configurer Ollama.

## ğŸš€ Installation d'Ollama

### Sur macOS
```bash
# Installation via Homebrew (recommandÃ©)
brew install ollama

# Ou tÃ©lÃ©charger depuis le site officiel
# https://ollama.ai/download
```

### Sur Linux
```bash
# Installation automatique
curl -fsSL https://ollama.ai/install.sh | sh
```

### Sur Windows
1. TÃ©lÃ©chargez l'installateur depuis https://ollama.ai/download
2. ExÃ©cutez l'installateur
3. RedÃ©marrez votre terminal

## ğŸ¯ Configuration

### 1. DÃ©marrer Ollama
```bash
# DÃ©marrer le service Ollama
ollama serve
```

### 2. TÃ©lÃ©charger un modÃ¨le
```bash
# ModÃ¨le recommandÃ© pour le franÃ§ais (lÃ©ger et efficace)
ollama pull llama3.2:3b

# Ou pour plus de puissance (nÃ©cessite plus de RAM)
ollama pull llama3.2:8b

# ModÃ¨le spÃ©cialisÃ© en franÃ§ais
ollama pull mistral:7b
```

### 3. VÃ©rifier l'installation
```bash
# Lister les modÃ¨les installÃ©s
ollama list

# Tester un modÃ¨le
ollama run llama3.2:3b
```

## ğŸ”§ Configuration dans SydoFlow

### Variables d'environnement
CrÃ©ez un fichier `.env.local` Ã  la racine du projet :

```env
# Configuration Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b
```

### VÃ©rification du statut
L'assistant IA affichera automatiquement le statut du LLM :
- ğŸŸ¢ **Vert** : LLM disponible et actif
- ğŸŸ¡ **Jaune** : LLM disponible mais non activÃ©
- ğŸ”´ **Rouge** : LLM non disponible

## ğŸ® Utilisation

### Commandes disponibles
- `"llm"` - VÃ©rifier le statut du LLM
- `"aide"` - Liste des commandes
- `"projets"` - Analyse des projets (avec enrichissement IA)
- `"Ã©quipe"` - Analyse de l'Ã©quipe (avec enrichissement IA)

### Fonctionnement hybride
L'assistant fonctionne en mode hybride :
1. **Analyse des donnÃ©es CSV** (toujours disponible)
2. **Enrichissement LLM** (si Ollama est actif)

## ğŸ› DÃ©pannage

### ProblÃ¨me : "LLM non disponible"
```bash
# VÃ©rifier que Ollama est dÃ©marrÃ©
ps aux | grep ollama

# RedÃ©marrer Ollama
ollama serve
```

### ProblÃ¨me : "ModÃ¨le non trouvÃ©"
```bash
# VÃ©rifier les modÃ¨les installÃ©s
ollama list

# Installer un modÃ¨le
ollama pull llama3.2:3b
```

### ProblÃ¨me : "Connexion refusÃ©e"
```bash
# VÃ©rifier que le port 11434 est libre
lsof -i :11434

# RedÃ©marrer Ollama
ollama serve
```

## ğŸ“Š ModÃ¨les recommandÃ©s

| ModÃ¨le | Taille | RAM requise | Performance | Usage |
|--------|--------|-------------|-------------|-------|
| `llama3.2:3b` | 2GB | 4GB | â­â­â­ | DÃ©veloppement |
| `llama3.2:8b` | 4.7GB | 8GB | â­â­â­â­ | Production |
| `mistral:7b` | 4.1GB | 8GB | â­â­â­â­ | FranÃ§ais |

## ğŸš€ DÃ©marrage rapide

```bash
# 1. Installer Ollama
brew install ollama

# 2. DÃ©marrer le service
ollama serve

# 3. Installer un modÃ¨le (dans un autre terminal)
ollama pull llama3.2:3b

# 4. DÃ©marrer SydoFlow
npm run dev
```

## ğŸ’¡ Conseils

- **DÃ©veloppement** : Utilisez `llama3.2:3b` (lÃ©ger et rapide)
- **Production** : Utilisez `llama3.2:8b` ou `mistral:7b` (plus performant)
- **MÃ©moire** : Assurez-vous d'avoir assez de RAM (4GB minimum pour 3b, 8GB pour 7b+)
- **Performance** : Le premier appel peut Ãªtre lent (chargement du modÃ¨le)

## ğŸ”„ Mise Ã  jour

```bash
# Mettre Ã  jour Ollama
brew upgrade ollama

# Mettre Ã  jour un modÃ¨le
ollama pull llama3.2:3b
```

---

**Note** : L'assistant fonctionne parfaitement sans LLM, mais l'enrichissement IA amÃ©liore considÃ©rablement la qualitÃ© des rÃ©ponses ! ğŸš€