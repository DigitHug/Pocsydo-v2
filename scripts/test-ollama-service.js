#!/usr/bin/env node

// Script de test pour v√©rifier le service OllamaService
import { Ollama } from 'ollama';

console.log('üß™ Test du service OllamaService...\n');

// Test 1: V√©rifier la connectivit√© Ollama
console.log('1Ô∏è‚É£ Test de connectivit√© Ollama...');
const testOllamaConnection = async () => {
  try {
    const ollama = new Ollama({
      host: 'http://localhost:11434'
    });
    
    const models = await ollama.list();
    console.log('‚úÖ Ollama accessible');
    console.log(`üì¶ Mod√®les disponibles: ${models.models.map(m => m.name).join(', ')}`);
    return true;
  } catch (error) {
    console.log('‚ùå Ollama non accessible:', error.message);
    return false;
  }
};

// Test 2: Test de g√©n√©ration de r√©ponse
console.log('\n2Ô∏è‚É£ Test de g√©n√©ration de r√©ponse...');
const testGeneration = async () => {
  try {
    const ollama = new Ollama({
      host: 'http://localhost:11434'
    });
    
    const response = await ollama.chat({
      model: 'llama3.2:3b',
      messages: [
        {
          role: 'system',
          content: 'Tu es un assistant IA sp√©cialis√© dans la gestion de projets. R√©ponds en fran√ßais de mani√®re concise et professionnelle.'
        },
        {
          role: 'user',
          content: 'Bonjour, comment √ßa va ?'
        }
      ],
      options: {
        temperature: 0.7,
        num_predict: 100
      }
    });

    console.log('‚úÖ G√©n√©ration de r√©ponse r√©ussie');
    console.log(`ü§ñ R√©ponse: "${response.message.content}"`);
    return true;
  } catch (error) {
    console.log('‚ùå Erreur de g√©n√©ration:', error.message);
    return false;
  }
};

// Test 3: Test avec contexte de projet
console.log('\n3Ô∏è‚É£ Test avec contexte de projet...');
const testProjectContext = async () => {
  try {
    const ollama = new Ollama({
      host: 'http://localhost:11434'
    });
    
    const systemPrompt = `Tu es un assistant IA sp√©cialis√© dans la gestion de projets et d'√©quipes. 
Tu es int√©gr√© dans un dashboard de gestion de projets appel√© "SydoFlow".

R√îLE:
- Assistant intelligent pour la gestion de projets
- Expert en analyse de donn√©es de projets et d'√©quipes
- Conseiller en organisation et planification

CONTEXTE:
Analyse des donn√©es actuelles:
üìä **√âtat des projets :**
‚Ä¢ 6 projets actifs
‚Ä¢ 3 projets en cours
‚Ä¢ 2 projets en review
‚Ä¢ 1 projet planifi√©

DONN√âES DISPONIBLES:
{
  "projets": [
    {
      "id": 1,
      "nom": "Refonte Site Web Luxe",
      "client": "Maison Martin",
      "statut": "En cours",
      "priorite": "Haute",
      "deadline": "2024-01-15",
      "responsable": "Sarah Martin",
      "progression": 75
    }
  ]
}

R√àGLES:
1. R√©ponds toujours en fran√ßais
2. Sois concis mais informatif
3. Utilise des emojis appropri√©s
4. Propose des actions concr√®tes
5. Base tes r√©ponses sur les donn√©es fournies

FORMAT DE R√âPONSE:
- Utilise des listes √† puces pour les points importants
- Mets en √©vidence les √©l√©ments critiques avec des emojis
- Propose des actions sp√©cifiques quand c'est pertinent

R√©ponds maintenant √† la question de l'utilisateur en gardant ces consignes √† l'esprit.`;

    const response = await ollama.chat({
      model: 'llama3.2:3b',
      messages: [
        {
          role: 'system',
          content: systemPrompt
        },
        {
          role: 'user',
          content: 'Peux-tu me donner un r√©sum√© de l\'√©tat des projets ?'
        }
      ],
      options: {
        temperature: 0.7,
        num_predict: 200
      }
    });

    console.log('‚úÖ G√©n√©ration avec contexte r√©ussie');
    console.log(`ü§ñ R√©ponse: "${response.message.content}"`);
    return true;
  } catch (error) {
    console.log('‚ùå Erreur de g√©n√©ration avec contexte:', error.message);
    return false;
  }
};

// Ex√©cuter tous les tests
async function runTests() {
  try {
    const test1 = await testOllamaConnection();
    const test2 = await testGeneration();
    const test3 = await testProjectContext();
    
    if (test1 && test2 && test3) {
      console.log('\nüéâ Tous les tests sont pass√©s !');
      console.log('\nüìã R√©sum√©:');
      console.log('‚Ä¢ ‚úÖ Ollama est accessible et fonctionnel');
      console.log('‚Ä¢ ‚úÖ Le mod√®le llama3.2:3b r√©pond correctement');
      console.log('‚Ä¢ ‚úÖ La g√©n√©ration avec contexte fonctionne');
      console.log('\nüöÄ Le service OllamaService est pr√™t !');
      console.log('\nüí° Le LLM devrait maintenant √™tre d√©tect√© dans le chatbot Discord');
    } else {
      console.log('\n‚ùå Certains tests ont √©chou√©');
      console.log('\nüîß Solutions possibles:');
      console.log('‚Ä¢ V√©rifiez qu\'Ollama est d√©marr√©: ollama serve');
      console.log('‚Ä¢ V√©rifiez que le mod√®le est install√©: ollama list');
      console.log('‚Ä¢ Red√©marrez l\'application: npm run dev');
    }
    
  } catch (error) {
    console.log('\n‚ùå Erreur lors des tests:', error.message);
  }
}

runTests();
