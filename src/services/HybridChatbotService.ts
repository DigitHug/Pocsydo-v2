import { chatbotIntelligence, ReponseIntelligente } from './ChatbotIntelligence';
import { ollamaService, OllamaResponse } from './OllamaService';
import { dataService } from './DataService';

export interface HybridResponse {
  message: string;
  type: 'info' | 'alerte' | 'suggestion' | 'analyse';
  suggestions?: string[];
  isLLMGenerated: boolean;
  data?: any;
}

class HybridChatbotService {
  private useLLM: boolean = false;
  private llmAvailable: boolean = false;

  async initialize(): Promise<void> {
    // V√©rifier si Ollama est disponible
    this.llmAvailable = await ollamaService.checkAvailability();
    this.useLLM = this.llmAvailable;
    
    console.log(`üß† LLM disponible: ${this.llmAvailable ? 'Oui' : 'Non'}`);
    
    if (this.llmAvailable) {
      console.log('‚úÖ LLM activ√© automatiquement !');
    } else {
      console.log('‚ö†Ô∏è LLM non disponible, utilisation du mode donn√©es uniquement');
    }
  }

  async processMessage(userMessage: string): Promise<HybridResponse> {
    // D'abord, analyser avec l'intelligence de donn√©es
    const dataAnalysis = await chatbotIntelligence.traiterMessage(userMessage);
    
    // Si le LLM est disponible, l'utiliser pour enrichir la r√©ponse
    if (this.useLLM && this.llmAvailable) {
      try {
        const context = this.buildContext(dataAnalysis);
        const projectData = this.getRelevantData(userMessage);
        
        const llmResponse = await ollamaService.generateResponse(
          userMessage, 
          context, 
          projectData
        );

        if (llmResponse.success) {
          return {
            message: llmResponse.message,
            type: dataAnalysis.type,
            suggestions: dataAnalysis.suggestions,
            isLLMGenerated: true,
            data: dataAnalysis.donnees
          };
        }
      } catch (error) {
        console.error('Erreur LLM, utilisation de l\'analyse de donn√©es:', error);
      }
    }

    // Fallback vers l'analyse de donn√©es
    return {
      message: dataAnalysis.message,
      type: dataAnalysis.type,
      suggestions: dataAnalysis.suggestions,
      isLLMGenerated: false,
      data: dataAnalysis.donnees
    };
  }

  private buildContext(dataAnalysis: ReponseIntelligente): string {
    let context = `Analyse des donn√©es actuelles:\n${dataAnalysis.message}\n\n`;
    
    if (dataAnalysis.suggestions && dataAnalysis.suggestions.length > 0) {
      context += `Suggestions identifi√©es:\n${dataAnalysis.suggestions.join('\n')}\n\n`;
    }

    if (dataAnalysis.donnees) {
      context += `Donn√©es pertinentes disponibles pour l'analyse.\n`;
    }

    return context;
  }

  private getRelevantData(userMessage: string): any {
    const messageLower = userMessage.toLowerCase();
    
    if (messageLower.includes('projet') || messageLower.includes('dossier')) {
      return {
        projets: dataService.getProjets(),
        projetsUrgents: dataService.getProjetsUrgents()
      };
    }
    
    if (messageLower.includes('√©quipe') || messageLower.includes('membre')) {
      return {
        equipe: dataService.getEquipe()
      };
    }

    return {
      projets: dataService.getProjets(),
      equipe: dataService.getEquipe()
    };
  }

  // M√©thodes de configuration
  toggleLLM(): boolean {
    if (this.llmAvailable) {
      this.useLLM = !this.useLLM;
      console.log(`üß† LLM ${this.useLLM ? 'activ√©' : 'd√©sactiv√©'}`);
    }
    return this.useLLM;
  }

  isLLMEnabled(): boolean {
    return this.useLLM && this.llmAvailable;
  }

  isLLMAvailable(): boolean {
    return this.llmAvailable;
  }

  async checkLLMStatus(): Promise<{ available: boolean; enabled: boolean }> {
    this.llmAvailable = await ollamaService.checkAvailability();
    return {
      available: this.llmAvailable,
      enabled: this.useLLM
    };
  }

  // M√©thodes pour g√©rer les mod√®les Ollama
  async getAvailableModels(): Promise<string[]> {
    if (this.llmAvailable) {
      return await ollamaService.getAvailableModels();
    }
    return [];
  }

  async pullModel(modelName: string): Promise<boolean> {
    if (this.llmAvailable) {
      return await ollamaService.pullModel(modelName);
    }
    return false;
  }

  setModel(modelName: string): void {
    if (this.llmAvailable) {
      ollamaService.setModel(modelName);
    }
  }
}

export const hybridChatbotService = new HybridChatbotService();

