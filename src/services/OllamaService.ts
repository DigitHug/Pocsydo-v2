// Service Ollama utilisant fetch pour √©viter les probl√®mes CORS

export interface OllamaResponse {
  success: boolean;
  message: string;
  error?: string;
}

export interface OllamaConfig {
  model: string;
  temperature: number;
  maxTokens: number;
}

class OllamaService {
  private config: OllamaConfig;
  private isAvailable: boolean = false;

  constructor() {
    this.config = {
      model: 'llama3.2:3b', // Mod√®le l√©ger et rapide
      temperature: 0.7,
      maxTokens: 1000
    };
  }

  async checkAvailability(): Promise<boolean> {
    try {
      // Test simple avec fetch pour √©viter les probl√®mes CORS
      const response = await fetch('http://localhost:11434/api/tags', {
        method: 'GET',
        headers: {
          'Content-Type': 'application/json',
        },
      });
      
      if (response.ok) {
        const data = await response.json();
        this.isAvailable = true;
        console.log('‚úÖ Ollama est disponible via fetch');
        console.log(`üì¶ Mod√®les disponibles: ${data.models?.map(m => m.name).join(', ') || 'Aucun'}`);
        return true;
      } else {
        this.isAvailable = false;
        console.log('‚ùå Ollama n\'est pas disponible (status:', response.status, ')');
        return false;
      }
    } catch (error) {
      this.isAvailable = false;
      console.log('‚ùå Ollama n\'est pas disponible:', error);
      return false;
    }
  }

  async generateResponse(
    userMessage: string, 
    context: string = '', 
    projectData?: any
  ): Promise<OllamaResponse> {
    if (!this.isAvailable) {
      return {
        success: false,
        message: 'Ollama n\'est pas disponible. Veuillez installer et d√©marrer Ollama.',
        error: 'OLLAMA_NOT_AVAILABLE'
      };
    }

    try {
      // Construire le prompt contextuel
      const systemPrompt = this.buildSystemPrompt(context, projectData);
      
      const response = await fetch('http://localhost:11434/api/generate', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: this.config.model,
          prompt: `${systemPrompt}\n\nUtilisateur: ${userMessage}\n\nAssistant:`,
          options: {
            temperature: this.config.temperature,
            num_predict: this.config.maxTokens
          }
        })
      });

      if (!response.ok) {
        throw new Error(`HTTP error! status: ${response.status}`);
      }

      const data = await response.json();
      
      return {
        success: true,
        message: data.response || 'R√©ponse g√©n√©r√©e avec succ√®s'
      };

    } catch (error) {
      console.error('Erreur Ollama:', error);
      return {
        success: false,
        message: 'Erreur lors de la g√©n√©ration de la r√©ponse',
        error: error instanceof Error ? error.message : 'UNKNOWN_ERROR'
      };
    }
  }

  private buildSystemPrompt(context: string, projectData?: any): string {
    let prompt = `Tu es un assistant IA sp√©cialis√© dans la gestion de projets et d'√©quipes. 
Tu es int√©gr√© dans un dashboard de gestion de projets appel√© "SydoFlow".

R√îLE:
- Assistant intelligent pour la gestion de projets
- Expert en analyse de donn√©es de projets et d'√©quipes
- Conseiller en organisation et planification

CONTEXTE:
${context}

DONN√âES DISPONIBLES:
${projectData ? JSON.stringify(projectData, null, 2) : 'Aucune donn√©e sp√©cifique'}

R√àGLES:
1. R√©ponds toujours en fran√ßais
2. Sois concis mais informatif
3. Utilise des emojis appropri√©s
4. Propose des actions concr√®tes
5. Base tes r√©ponses sur les donn√©es fournies
6. Si tu n'as pas assez d'informations, demande des pr√©cisions
7. Reste professionnel mais accessible

FORMAT DE R√âPONSE:
- Utilise des listes √† puces pour les points importants
- Mets en √©vidence les √©l√©ments critiques avec des emojis
- Propose des actions sp√©cifiques quand c'est pertinent

Exemples de r√©ponses:
- "üìä Voici l'analyse de vos projets..."
- "‚ö†Ô∏è Attention: 3 projets sont en retard"
- "üí° Je recommande de..."

R√©ponds maintenant √† la question de l'utilisateur en gardant ces consignes √† l'esprit.`;

    return prompt;
  }

  async getAvailableModels(): Promise<string[]> {
    try {
      const response = await fetch('http://localhost:11434/api/tags', {
        method: 'GET',
        headers: {
          'Content-Type': 'application/json',
        },
      });
      
      if (response.ok) {
        const data = await response.json();
        return data.models?.map((model: any) => model.name) || [];
      }
      return [];
    } catch (error) {
      console.error('Erreur lors de la r√©cup√©ration des mod√®les:', error);
      return [];
    }
  }

  async pullModel(modelName: string): Promise<boolean> {
    try {
      console.log(`T√©l√©chargement du mod√®le ${modelName}...`);
      
      const response = await fetch('http://localhost:11434/api/pull', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          name: modelName
        })
      });
      
      if (response.ok) {
        console.log(`‚úÖ Mod√®le ${modelName} t√©l√©charg√© avec succ√®s`);
        return true;
      } else {
        console.error(`‚ùå Erreur lors du t√©l√©chargement du mod√®le ${modelName}:`, response.status);
        return false;
      }
    } catch (error) {
      console.error(`‚ùå Erreur lors du t√©l√©chargement du mod√®le ${modelName}:`, error);
      return false;
    }
  }

  setModel(modelName: string): void {
    this.config.model = modelName;
  }

  setTemperature(temperature: number): void {
    this.config.temperature = Math.max(0, Math.min(2, temperature));
  }

  getConfig(): OllamaConfig {
    return { ...this.config };
  }

  isOllamaAvailable(): boolean {
    return this.isAvailable;
  }
}

export const ollamaService = new OllamaService();

